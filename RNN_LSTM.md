# RNN과 LSTM

# 0. What is recurrent?
>recurrnet.  
>되풀이되는, 반복되는

기존까지 우리가 배워왔던 피드포워드 신경망들에서 입력 데이터는 모든 노드를 각 한번씩만 지나가게 된다. 한번만 지나가기 때문에 데이터에서 시간의 개념을 제외하고 독립적으로 학습하기 때문에 기억이란것이 존재하지 않고, 현재 들어온 데이터만을 고려한다.  
그러나 이 글에서 살펴볼 순환 신경망들은 다르다. 순환 신경망들은 지금 들어온 입력 데이터와 이전에 들어온 데이터를 같이 고려한다.  
그 뜻은 과거가 현재에 영향을 준다는 것이고 기억이 존재한다는 것이다.

# 1. RNNs

RNN 모델은 앞의 순환 신경망 개념이 실제로 적용된 아키텍처이다.

RNN은 다음과 같은 그림으로 설명 가능하다. 여기서 ``t``는 시간이다.
``X_t``를 입력으로 받아 ``h_t``를 출력으로 내놓고 ``h_t``가 다시 다음 입력을 위해 사용된다.  

좀더 쉬운 이해를 위해 시간순 나열이 된  다음 그림으로 설명해 보자면 좀 더 쉬울 것이다. 시간 0에서 나온 출력이 시간 1의 노드에 영향을 미치고, 그렇게 나온 출력이 또 시간 2의 노드에 영향을 미치는 식이다.

여기서 보이는 뉴럴넷 A는 다 같다는것을 꼭 명심해 두어야 한다. 이것이 바로 자기 자신에게 과거 자기 자신의 출력을 현재 자신의 출력에 영향을 주는것의 핵심이다.

# 2. Vanishing Gradient

Vanishing Gradient 문제는 전에 써 놓았던 ReLU 함수에서 잠깐 다뤘었다. sigmoid 함수가 여러번 곱해지면 0에 수렴하게 되어 gradient가 사라지는 문제점이었는데 RNNs 또한 오류역전파를 기본으로 만든 Backpropagation Through Time을 사용하기 때문에 동일한 문제가 발생하게 된다. 이 말은 곧 깊은 기억으로 거슬러 올라갈 수 없다는 뜻이며 많은 제약이 발생한다는 것이다.

# 3. LSTMs

LSTM은 위의 문제를 해결하였다. LSTM은 본질적으로는 RNN과 같은 개념을 공유한다. 그러나 기존 RNN은 아주 단순한 tanh 레이어만 존재했다면 RNN은 좀 더 발전되어 셀 스테이트와 게이트라는것이 존재한다.

# 4. Structure of LSTM

RNN의 핵심은 셀 스테이트다.