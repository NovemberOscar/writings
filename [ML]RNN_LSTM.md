# RNN과 LSTM

# 0. What is recurrent?
>recurrnet.  
>되풀이되는, 반복되는

기존까지 우리가 배워왔던 피드포워드 신경망들에서 입력 데이터는 모든 노드를 각 한번씩만 지나가게 된다. 한번만 지나가기 때문에 데이터에서 시간의 개념을 제외하고 독립적으로 학습하기 때문에 기억이란것이 존재하지 않고, 현재 들어온 데이터만을 고려한다.  
그러나 이 글에서 살펴볼 순환 신경망들은 다르다. 순환 신경망들은 지금 들어온 입력 데이터와 이전에 들어온 데이터를 같이 고려한다.  
그 뜻은 과거가 현재에 영향을 준다는 것이고 기억이 존재한다는 것이다.

# 1. RNNs

RNN 모델은 앞의 순환 신경망 개념이 실제로 적용된 아키텍처이다.

RNN은 다음과 같은 그림으로 설명 가능하다. 여기서 ``t``는 시간이다.
``X_t``를 입력으로 받아 ``h_t``를 출력으로 내놓고 ``h_t``가 다시 다음 입력을 위해 사용된다.  

좀더 쉬운 이해를 위해 시간순 나열이 된  다음 그림으로 설명해 보자면 좀 더 쉬울 것이다. 시간 0에서 나온 출력이 시간 1의 노드에 영향을 미치고, 그렇게 나온 출력이 또 시간 2의 노드에 영향을 미치는 식이다.

여기서 보이는 뉴럴넷 A는 다 같다는것을 꼭 명심해 두어야 한다. 이것이 바로 자기 자신에게 과거 자기 자신의 출력을 현재 자신의 출력에 영향을 주는것의 핵심이다.

# 2. Vanishing Gradient

Vanishing Gradient 문제는 전에 써 놓았던 ReLU 함수에서 잠깐 다뤘었다. sigmoid 함수가 여러번 곱해지면 0에 수렴하게 되어 gradient가 사라지는 문제점이었는데 RNNs 또한 오류역전파를 기본으로 만든 Backpropagation Through Time을 사용하기 때문에 동일한 문제가 발생하게 된다. 이 말은 곧 깊은 기억(장기 의존성)으로 거슬러 올라갈 수 없다는 뜻이며 문맥 참조같은 기능에서 많은 제약이 발생한다는 것이다.

# 3. LSTMs

LSTM은 본질적으로는 RNN과 같은 개념을 공유한다. 그러나 기존 RNN은 아주 단순한 tanh 레이어만 존재했다면 RNN은 좀 더 발전되어 셀 스테이트와 게이트라는것이 존재한다. 그리고 이 게이트와 셀 스테이트를 이용한 설계 자체가 Vanishing Gradient 문제를 해결하여 장기 의존성을 높이는 방식이기 때문에 LSTM은 위의 문제를 해결할 수 있었다.

# 4. Structure of LSTM

LSTM의 핵심은 ``셀 스테이트``다. ``셀 스테이트``는 LSTM 체인 전체를 관통하면서 LSTM의 스테이트(기억)를 관리한다. 또한 LSTM에는 셀 스테이트에 흐르는 정보를 관리하기 위해 ``게이트``라는 것을 활용한다.

게이트들은 셀 스테이트에 정보를 더하거나 제거하는 역할을 한다. LSTM의 게이트는 총 3개이다.

1. 망각 게이트  
LSTM 게이트의 첫 단계는 셀 스테이트를 얼마나 유지할지를 결정하는 역할이다. 입력받은 h_t-1과 x_t를 시그모이드 레이어에 통과시키면 0 과 1사이의 값이 나오고 이 망각 게이트의 출력이 셀 스테이트에 곱해져서 셀 스테이트를 얼마나 유지할지를 결정한다.

2. 입력 게이트  
망각 게이트는 셀 스테이트에 대한 유지를 관리하는 게이트였다. 입력 게이트는 입력된 데이터가 셀 스테이트에 얼마나 반영될지를 결정하는 게이트이다. 입력 게이트는 두개의 레이어를 가진다. 첫번쨰 sigmoid 레이어는 이 입력을 셀 스테이트에 반영할지 결정한다. 그리고 두번째 tanh 레이어는 셀 스테이트에 더해질 값을 조정하는 역할이다(기존 RNN의 값 업데이트 방식과 동일) 그리고 두 레이어의 출력을 곱해서 셀 스테이트에 반영한다. 

입력 게이트를 셀 스테이트에 반영함으로써 셀 스테이트는 과거 상태에서 현재 상태로 완성된다.

3. 출력 게이트
이제 마지막 게이트이다. 앞의 두 게이트에서는 셀 스테이트와 입력값을 조정했다. 출력 게이트는 입력값을 시그모이드 레이어로 필터링한 값과 앞에서 업데이트된 셀 스테이트를 tanh 레이어에 통과시킨 값을 곱해서 현재까지의 문맥을 고려한 셀 스테이트의 값을 출력으로 내보낸다. 

